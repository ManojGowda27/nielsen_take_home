{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = 'take_home_assessment.xlsx'\n",
    "q1_data = pd.read_excel(file_path, sheet_name='Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract actual and predicted values for Q1\n",
    "y_actual = q1_data['y_actual']\n",
    "y_pred_model_1 = q1_data['y_predicted_model_1']\n",
    "y_pred_model_2 = q1_data['y_predicted_model_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics for Model 1\n",
    "metrics_model_1 = {\n",
    "    \"Accuracy\": accuracy_score(y_actual, y_pred_model_1),\n",
    "    \"Precision\": precision_score(y_actual, y_pred_model_1),\n",
    "    \"Recall\": recall_score(y_actual, y_pred_model_1),\n",
    "    \"F1-Score\": f1_score(y_actual, y_pred_model_1)\n",
    "}\n",
    "\n",
    "# Calculate performance metrics for Model 2\n",
    "metrics_model_2 = {\n",
    "    \"Accuracy\": accuracy_score(y_actual, y_pred_model_2),\n",
    "    \"Precision\": precision_score(y_actual, y_pred_model_2),\n",
    "    \"Recall\": recall_score(y_actual, y_pred_model_2),\n",
    "    \"F1-Score\": f1_score(y_actual, y_pred_model_2)\n",
    "}\n",
    "\n",
    "# Combine metrics for comparison\n",
    "metrics_comparison = pd.DataFrame([metrics_model_1, metrics_model_2], index=[\"Model 1\", \"Model 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Accuracy  Precision  Recall  F1-Score\n",
      "Model 1     0.700   0.444444     0.8  0.571429\n",
      "Model 2     0.825   0.800000     0.4  0.533333\n"
     ]
    }
   ],
   "source": [
    "# Display the comparison table for Q1 metrics\n",
    "print(metrics_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model 1 if:\n",
    "\n",
    "The application prioritizes recall, meaning it’s crucial to identify as many true positives as possible (e.g., disease diagnosis, fraud detection). Missing a positive case is costlier than having a few false positives.\n",
    "## Choose Model 2 if:\n",
    "\n",
    "The application prioritizes precision and overall accuracy, meaning it’s important to minimize false positives or the total number of incorrect predictions (e.g., spam detection, financial approvals). Making an incorrect positive prediction is more detrimental.\n",
    "## Recommendation:\n",
    "If the application generally values balanced performance and reducing incorrect predictions (as indicated by accuracy and precision), Model 2 is likely the better choice. However, in scenarios where identifying all positives is critical, Model 1 should be preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NTKH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
