**Note**: The calculations for entropy, weighted entropy, and information gain are documented in **Sheet Q3** of the file `take_home_assessment.xlsx`.

The final decision tree is constructed and represented in **Sheet 3a - Final Decision Tree** of the same file.


## 3a:
## 1. Context
The objective of this task was to build a decision tree to classify whether a customer buys a product or not, based on attributes such as `age_group`, `income`, and `credit_rating`. My approach involved:
- Calculating Entropy and Information Gain (IG) for each attribute.
- Selecting the best attribute splits to construct a decision tree.

---

## 2. Approach

### Step 1: Understanding the Data
I analyzed the dataset to understand the distribution of values for the attributes and the target variable (`buys_product`).

### Step 2: Calculating Entropy and Information Gain
1. **Entropy**: Measures uncertainty within a group. The formula is:
   ```
   H = -p(Yes) * log2(p(Yes)) - p(No) * log2(p(No))
   ```
   Where:
   - \( p(Yes) \): Proportion of "Yes" outcomes.
   - \( p(No) \): Proportion of "No" outcomes.

2. **Information Gain**: Measures the reduction in entropy after splitting by an attribute. The formula is:
   ```
   IG = H(Target) - H(Attribute)
   ```

### Step 3: Constructing the Decision Tree
1. **Root Node**:
   - The attribute with the highest Information Gain becomes the root node.
2. **Splitting**:
   - For each branch with non-zero entropy (mixed outcomes), further splits are made using the attribute with the highest Information Gain.
3. **Stopping Criteria**:
   - If entropy = 0 (pure outcomes), a leaf node is created.

---

### **Information Gain and Entropy Tables**

#### **Values for Income**
| **Group** | **Count** | **Entropy** | **Weight**        |
|-----------|-----------|-------------|-------------------|
| High      | 4         | 1.000       | 4/14 = 0.286      |
| Medium    | 6         | 0.918       | 6/14 = 0.429      |
| Low       | 4         | 0.811       | 4/14 = 0.286      |

- **Weighted Entropy (WE)**: 0.911
- **Information Gain (IG)**: 0.032

---

#### **Values for Credit_Rating**
| **Group**      | **Count** | **Entropy** | **Weight**        |
|----------------|-----------|-------------|-------------------|
| Excellent      | 6         | 1.000       | 6/14 = 0.429      |
| Fair           | 8         | 0.811       | 8/14 = 0.571      |

- **Weighted Entropy (WE)**: 0.892
- **Information Gain (IG)**: 0.051

---

#### **Values for Age_Group**
| **Age_Group** | **Count** | **Entropy** |
|---------------|-----------|-------------|
| 18-24         | 6         | 1.000       |
| 25-44         | 3         | 0.000       |
| 45+           | 5         | 0.970       |

- **Weighted Entropy (WE)**: 0.7754
- **Information Gain (IG)**: 0.1683

---


## 3. Decision Tree Construction

### Root Node
- **Attribute with Highest IG**: `age_group` (⁠⁠**IG = 0.1683**).
- **Branches**:
  - `age_group = 18-24`
  - `age_group = 25-44`
  - `age_group = 45+`

### Further Splits

#### Branch: `age_group = 18-24`
- **Next Best Attribute**: `credit_rating` (⁠⁠**IG = 0.051**).
  - **Split by `credit_rating`**:
    - `credit_rating = Excellent`:
      - Further split by `income` (⁠⁠**IG = 0.032**).
        - `income = High`: Leaf node (⁠⁠**buys_product = No**).
        - `income = Medium`: Leaf node (⁠⁠**buys_product = Yes**).
        - `income = Low`: Leaf node (⁠⁠**buys_product = Yes**).
    - `credit_rating = Fair`: Leaf node (⁠⁠**buys_product = No**).

#### Branch: `age_group = 25-44`
- Entropy = 0.000 (pure outcome: all "Yes").
- Leaf node (⁠⁠**buys_product = Yes**).

#### Branch: `age_group = 45+`
- **Next Best Attribute**: `credit_rating` (⁠⁠**IG = 0.051**).
  - **Split by `credit_rating`**:
    - `credit_rating = Excellent`: Leaf node (⁠⁠**buys_product = No**).
    - `credit_rating = Fair`: Leaf node (⁠⁠**buys_product = Yes**).

---

## 4. Final Decision Tree

```
age_group
├── 18-24
│   ├── credit_rating = Excellent
│   │   ├── income = High → No
│   │   ├── income = Medium → Yes
│   │   ├── income = Low → Yes
│   ├── credit_rating = Fair → No
├── 25-44 → Yes
├── 45+
    ├── credit_rating = Excellent → No
    ├── credit_rating = Fair → Yes
```

---

## 5. Justifications

### Attribute Selection
- **Why `age_group` as the root node?**
  - It had the highest Information Gain (⁠⁠**IG = 0.1683**), making it the best attribute to reduce uncertainty.

- **Why `credit_rating` next?**
  - For branches with mixed outcomes (`18-24` and `45+`), `credit_rating` had the highest IG (⁠⁠**IG = 0.051**).

- **Why `income` last?**
  - `Income` had the lowest IG (⁠⁠**IG = 0.032**) but was still useful for refining splits when other attributes were exhausted.

### Challenges Addressed
- **Handling Mixed Outcomes**: Used Information Gain to guide splits in branches with non-zero entropy.
- **Avoiding Overfitting**: Stopped splitting when entropy = 0 (pure outcomes) or all attributes were used.

---

## 6. Outcomes
- **Accuracy**: The decision tree ensures correct classification based on the given data.
- **Interpretability**: The tree is easy to understand and explain, with clear rules at each split.
- **Applicability**: This model can be applied to new customers to predict whether they will buy the product.

---

## 7. Key Insights for the Interviewer
1. My decision-making was guided by clear mathematical principles (entropy and IG).
2. I balanced precision and simplicity to construct an interpretable tree.
3. This approach is scalable for larger datasets and additional attributes.

---

## 3b:
### **Alternative Splitting Criteria for Decision Trees**

In addition to **Information Gain**, we could use the following splitting criteria:

1. **Gini Index**:
   - Measures the impurity of a dataset.
   - Formula:
     ```
     G = 1 - ∑ p(i)^2
     ```
   - Lower Gini Index indicates a better split.
   - Commonly used in CART (Classification and Regression Trees).

2. **Gain Ratio**:
   - Adjusts Information Gain by accounting for the number of branches (bias towards attributes with many values).
   - Formula:
     ```
     Gain Ratio = Information Gain / Split Information
     ```
   - Useful when attributes have a large number of distinct values.

3. **Chi-Square Statistic**:
   - Measures the statistical significance of the split.
   - Compares observed and expected frequencies to determine the strength of association.
   - Suitable for categorical data.

4. **Reduction in Variance** (for Regression Trees):
   - Minimizes the variance in the target variable after the split.
   - Focuses on numerical outcomes rather than categorical.

---

### **Why These Could Be Chosen?**
- **Gini Index**: Simpler to compute and effective for balanced datasets.
- **Gain Ratio**: Addresses bias in attributes with many values, providing fairer splits.
- **Chi-Square**: Ensures splits are statistically significant.
- **Reduction in Variance**: Ideal for regression tasks.

These criteria could be chosen based on the nature of the dataset, target variable, and the problem's requirements.

---

### **Question 3(c): Predict the Outcome**

We are tasked to predict the outcome for the following user based on the decision tree:

| **user_id** | **age_group** | **income** | **credit_rating** | **buys_product** |
|-------------|---------------|------------|-------------------|------------------|
| 15          | 45+           | low        | excellent         | ?                |

---

### **Using the Final Decision Tree**

#### **Step 1: Start at the Root Node**
The root node is **`age_group`**.

- For `user_id = 15`, `age_group = 45+`.
- Follow the branch for `age_group = 45+`.

---

#### **Step 2: Split by `credit_rating`**
For `age_group = 45+`, the next split is on **`credit_rating`**.

- For `user_id = 15`, `credit_rating = excellent`.
- Follow the branch for `credit_rating = excellent`.

---

#### **Step 3: Leaf Node**
For `age_group = 45+` and `credit_rating = excellent`, the decision tree shows:

- **`buys_product = No`**.

---

### **Final Prediction**
For `user_id = 15`:
- **`buys_product = No`**

---

